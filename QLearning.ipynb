{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> <p style=\"text-align: center;\"> Simple Reinforcement Learning in Python</p></h1> \n",
    "\n",
    "<img src=\"imgs/game_intro.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import randint as rand\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import copy\n",
    "\n",
    "import tqdm.notebook\n",
    "from IPython.display import clear_output\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Maze\n",
    "\n",
    "class Maze:\n",
    "    \n",
    "    def __init__(self, width=81, height=51, complexity=.75, density=.75):\n",
    "        \n",
    "        # make a maze, convert array from True/False to integer:\n",
    "        self.z = np.array(self.build_maze(width, height, complexity, density), dtype='int')\n",
    "        self.shape = self.z.shape\n",
    "        \n",
    "        # Place Target/Reward\n",
    "        self.target_image = plt.imread('imgs/bread.png') \n",
    "        self.target_x, self.target_y = self.random_availale_ij()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return f\"Maze {self.shape} with target at {(self.target_x, self.target_y)}\\n\\n{self.z}\"\n",
    "        \n",
    "    # From Wikipedia\n",
    "    def build_maze(self, width, height, complexity, density):\n",
    "        # Only odd shapes\n",
    "        shape = ((height // 2) * 2 + 1, (width // 2) * 2 + 1)\n",
    "        # Adjust complexity and density relative to maze size\n",
    "        complexity = int(complexity * (5 * (shape[0] + shape[1]))) # number of components\n",
    "        density    = int(density * ((shape[0] // 2) * (shape[1] // 2))) # size of components\n",
    "        # Build actual maze\n",
    "        Z = np.zeros(shape, dtype=bool)\n",
    "        # Fill borders\n",
    "        Z[0, :] = Z[-1, :] = 1\n",
    "        Z[:, 0] = Z[:, -1] = 1\n",
    "        # Make aisles\n",
    "        for i in range(density):\n",
    "            x, y = rand(0, shape[1] // 2) * 2, rand(0, shape[0] // 2) * 2 # pick a random position\n",
    "            Z[y, x] = 1\n",
    "            for j in range(complexity):\n",
    "                neighbours = []\n",
    "                if x > 1:             neighbours.append((y, x - 2))\n",
    "                if x < shape[1] - 2:  neighbours.append((y, x + 2))\n",
    "                if y > 1:             neighbours.append((y - 2, x))\n",
    "                if y < shape[0] - 2:  neighbours.append((y + 2, x))\n",
    "                if len(neighbours):\n",
    "                    y_,x_ = neighbours[rand(0, len(neighbours) - 1)]\n",
    "                    if Z[y_, x_] == 0:\n",
    "                        Z[y_, x_] = 1\n",
    "                        Z[y_ + (y - y_) // 2, x_ + (x - x_) // 2] = 1\n",
    "                        x, y = x_, y_\n",
    "        return Z\n",
    "    \n",
    "    def random_availale_ij(self):\n",
    "        \n",
    "        i, j = np.random.choice(self.shape[0]), np.random.choice(self.shape[1])\n",
    "        while self.z[i][j] != 0:\n",
    "            i, j = np.random.choice(self.shape[0]), np.random.choice(self.shape[1])\n",
    "        \n",
    "        return i, j\n",
    "    \n",
    "\n",
    "    def render_target(self, ax):\n",
    "        \n",
    "        target_ico = OffsetImage(self.target_image, zoom=5/ax.figure.dpi, dpi_cor=False)\n",
    "        target_ab = AnnotationBbox(target_ico, (self.target_y, self.target_x), frameon=False)  # flipped x/y\n",
    "        ax.add_artist(target_ab)\n",
    "        return ax\n",
    "    \n",
    "    def render(self, ax=None):\n",
    "        \n",
    "        if ax is None:\n",
    "            fig = plt.figure(figsize=(6,6)) \n",
    "            ax = plt.gca()\n",
    "            \n",
    "        ax.imshow(self.z, origin='upper', vmin=0, vmax=3)\n",
    "        ax = self.render_target(ax)\n",
    "        \n",
    "        ax.axis('off')\n",
    " \n",
    "        return ax\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, maze, q=None):\n",
    "        \n",
    "        self.maze = maze\n",
    "        \n",
    "        self.x, self.y = self.maze.random_availale_ij()\n",
    "        while (self.x, self.y) == (self.maze.target_x, self.maze.target_y):\n",
    "            self.x, self.y = self.maze.random_availale_ij()\n",
    "        \n",
    "        self.x_prev, self.y_prev = (None, None)\n",
    "        \n",
    "        if q is None:\n",
    "            self.q = np.zeros([maze.shape[0], maze.shape[1], 4])\n",
    "        else:\n",
    "            self.q = q\n",
    "        \n",
    "        self.a = None\n",
    "        self.r = 0\n",
    "        \n",
    "        self.on_target = False\n",
    "        \n",
    "        # agent icon\n",
    "        self.agent_image = plt.imread('imgs/emoji.png') \n",
    "        \n",
    "            \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return f\"Agent now at {(self.x, self.y)} took action {self.a} from {(self.x_prev, self.y_prev)} and got rewarded {self.r} | Policy table {self.q.shape}\"\n",
    "    \n",
    "    def available_actions(self):\n",
    "        \n",
    "        d = {(-1, 0):0, (1, 0):1, (0, 1):2, (0, -1):3}  # U, D, R, L\n",
    "        \n",
    "        # All actions available unless lead to wall\n",
    "        available_a = [d[k] for k in d if self.maze.z[self.x + k[0], self.y + k[1]] == 0]\n",
    "\n",
    "        return available_a\n",
    "        \n",
    "    def pick_action(self, eps):\n",
    "        \n",
    "        actions = self.available_actions()\n",
    "        \n",
    "        strategy = np.random.random()\n",
    "    \n",
    "        if strategy < eps:\n",
    "            # Random Exploaration\n",
    "            self.a = np.random.choice(actions)\n",
    "        \n",
    "        else:\n",
    "            # Table Exploit\n",
    "            avaq = [self.q[self.x, self.y, ava] for ava in actions] # find Q of available actions\n",
    "            self.a = actions[np.argmax(avaq)] # select available action with maximum Q\n",
    "\n",
    "    \n",
    "    def execute_action(self):\n",
    "        \n",
    "        mov = {0:(-1, 0), 1:(1, 0), 2:(0, 1), 3:(0, -1)}  # U, D, R, L\n",
    "        \n",
    "        self.x_prev = self.x\n",
    "        self.y_prev = self.y\n",
    "        \n",
    "        self.x += mov[self.a][0]\n",
    "        self.y += mov[self.a][1]\n",
    "        \n",
    "        \n",
    "    def move(self, eps):\n",
    "        \n",
    "        self.pick_action(eps)\n",
    "        \n",
    "        self.execute_action()\n",
    "        \n",
    "        if (self.x, self.y) == (self.maze.target_x, self.maze.target_y):\n",
    "            self.on_target = True\n",
    "      \n",
    "    def reward(self):\n",
    "        \n",
    "        if self.on_target:\n",
    "            r = 10\n",
    "        else:\n",
    "            r = 0\n",
    "        \n",
    "        self.r = r\n",
    "        \n",
    "    def update_q(self, alpha, gamma):\n",
    "        \n",
    "        q_max = np.max(self.q[self.x, self.y, :]) # maximum Q that I can see from this new position      \n",
    "            \n",
    "        self.q[self.x_prev, self.y_prev, self.a] = (1 - alpha)*self.q[self.x_prev, self.y_prev, self.a] + alpha*(self.r + gamma*q_max)\n",
    "      \n",
    "    \n",
    "    def respawn(self):\n",
    "        \n",
    "        new_agent = Agent(self.maze, self.q)\n",
    "        \n",
    "        return new_agent\n",
    "        \n",
    "        \n",
    "    def render_q(self, axs=None):\n",
    "        \"\"\"\n",
    "        Visualization of the policy table for each action\n",
    "        \"\"\"\n",
    "        vmax = np.max(self.q)\n",
    "        \n",
    "        if axs is None:\n",
    "            fig, axs = plt.subplots(1, 5, figsize=(12,3))\n",
    "        \n",
    "        # Individual actions Q-values\n",
    "        for i, x in enumerate(axs[:-1]):\n",
    "            dirs = {0:\"UP\", 1:\"DOWN\", 2:\"RIGHT\", 3:\"LEFT\"} # U, D, R, L\n",
    "            x.imshow(self.q[:,:,i] - self.maze.z, origin='upper', vmin=-1, vmax=vmax)  # also see maze walls\n",
    "            x.set_title(f\"{dirs[i]}\")\n",
    "            x.axis('off')\n",
    "        \n",
    "        # Best Q-Value for any given state\n",
    "        axs[-1].imshow(np.max(self.q, axis=-1) - self.maze.z, origin='upper', vmin=-1, vmax=vmax)\n",
    "        axs[-1].axis('off')\n",
    "        \n",
    "        return axs\n",
    "         \n",
    "    def render(self, ax=None):\n",
    "        \n",
    "        if ax is None:\n",
    "            fig = plt.figure(figsize=(6,6)) \n",
    "            ax = plt.gca()\n",
    "            \n",
    "        agent_ico = OffsetImage(self.agent_image, zoom=4/ax.figure.dpi, dpi_cor=False)\n",
    "        agent_ab = AnnotationBbox(agent_ico, (self.y, self.x), frameon=False)  # flipped x/y\n",
    "        ax.add_artist(agent_ab)\n",
    "        \n",
    "        return ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dashboard(agent, maze, epoch=0):\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 9))\n",
    "    fig.suptitle(f\"Epoch {epoch}\")\n",
    "\n",
    "    # Blueprint\n",
    "    gs1 = GridSpec(3, 5)\n",
    "    ax1 = fig.add_subplot(gs1[0:2, 0:2])\n",
    "    ax2 = fig.add_subplot(gs1[0:2, 2:4])\n",
    "    ax3 = fig.add_subplot(gs1[-1, :2])\n",
    "    ax4 = fig.add_subplot(gs1[-1, 2])\n",
    "    ax5 = fig.add_subplot(gs1[-1, 3])\n",
    "    ax6 = fig.add_subplot(gs1[0:1, 4])\n",
    "    ax7 = fig.add_subplot(gs1[1:2, 4])\n",
    "    ax8 = fig.add_subplot(gs1[-1, 4])\n",
    "\n",
    "    # Maze\n",
    "    ax1 = maze.render(ax1)\n",
    "    ax1 = agent.render(ax1)\n",
    "    \n",
    "    # Brains\n",
    "    axs = [ax6, ax7, ax5, ax4, ax2]  # U, D, R, L, TOT\n",
    "    axs = agent.render_q(axs)\n",
    "    \n",
    "    # Q at given state\n",
    "    # -- normalise by max Q in brain\n",
    "    vmax = np.max(agent.q)\n",
    "    # -- roll array so to have actions in human-friendly order L,U,D,R\n",
    "    qs_here = [np.array(agent.q[agent.x, agent.y, :])]\n",
    "    ax3.imshow(np.roll(qs_here, 1), vmin=-1, vmax=vmax)\n",
    "    # -- Set plot ticks\n",
    "    ax3.set_yticks([])\n",
    "    ax3.set_xticks(range(4))\n",
    "    ax3.set_xticklabels([\"L\", \"U\", \"D\", \"R\"],fontsize=16)  # note we rolled values\n",
    "    # -- Remove axes framse\n",
    "    ax3.spines['top'].set_visible(False)\n",
    "    ax3.spines['right'].set_visible(False)\n",
    "    ax3.spines['bottom'].set_visible(False)\n",
    "    ax3.spines['left'].set_visible(False)\n",
    "\n",
    "    # Avatar icon\n",
    "    ax8.imshow(agent.agent_image)\n",
    "    ax8.axis(\"off\")\n",
    "    \n",
    "    return ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8 \n",
    "  \n",
    "# Exponential decay exploration rate update at episode i\n",
    "def update_eps(i, min_eps, max_eps, eps_tau):\n",
    "    \n",
    "    return min_eps + (max_eps - min_eps) * np.exp(-i/eps_tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "maze_w = 32\n",
    "maze_h = 32\n",
    "\n",
    "# Training Parameters\n",
    "alpha = 0.1  # Learning rate, keep low if evolving in stochastic environments (0.1)\n",
    "gamma = 0.9  # Reward glow (0.9)\n",
    "epochs = 6000  # number of epochs to train for (6000)\n",
    "max_steps = 3000 # maximum number of steps agent has available per epoch (3000)\n",
    "#save_epochs = np.logspace(0, np.log10(epochs), 10, dtype=int)  # epoch at which to save agent brain\n",
    "\n",
    "# Exploration / Exploitation parameters\n",
    "max_eps = 1.0  \n",
    "min_eps = 0.1\n",
    "eps_tau = epochs/4  # 1/e exploration rate @ eps_gamma epochs\n",
    "\n",
    "\n",
    "# INIT\n",
    "#qBank = []\n",
    "\n",
    "# Create a fixed random maze\n",
    "world = Maze(width=maze_w, height=maze_h)\n",
    "ax = world.render()\n",
    "\n",
    "# Init Agent with no experience\n",
    "agent = Agent(world)\n",
    "\n",
    "# TRAIN\n",
    "for epoch in tqdm.notebook.tqdm(range(epochs)):\n",
    "    \n",
    "    # Set Exploration rate of agent in this epoch\n",
    "    eps = update_eps(epoch, min_eps, max_eps, eps_tau)\n",
    "\n",
    "    for s in range(max_steps):\n",
    "\n",
    "        agent.move(eps)\n",
    "        agent.reward()\n",
    "        agent.update_q(alpha, gamma)\n",
    "            \n",
    "        # Plot every step if specific epoch\n",
    "        if epoch+1 == epochs:\n",
    "            \n",
    "            axs = plot_dashboard(agent, world, epoch)\n",
    "            plt.show()\n",
    "\n",
    "            sleep(0.1)\n",
    "            if not agent.on_target: clear_output(wait=True)\n",
    "        \n",
    "        # Stop stepping if on target\n",
    "        if agent.on_target:\n",
    "            break\n",
    "    \n",
    "    # Save agent in database\n",
    "    #if epoch in save_epochs:\n",
    "        #qBank.append(copy.deepcopy(agent.q))\n",
    "     \n",
    "    # Reset agent with newly acquired experience \n",
    "    if epoch+1 != epochs:\n",
    "        agent = agent.respawn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Show off\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    clone = copy.deepcopy(Agent(world, agent.q))\n",
    "\n",
    "    # Show off agent\n",
    "    while not clone.on_target:\n",
    "\n",
    "        clone.move(0)  # move in complete table-exploit\n",
    "        axs = plot_dashboard(clone, world)\n",
    "        plt.show()\n",
    "\n",
    "        sleep(0.1)\n",
    "        clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vdsp-esi",
   "language": "python",
   "name": "vdsp-esi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
